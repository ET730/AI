{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c707f35e",
   "metadata": {},
   "source": [
    "# AI Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233976dd",
   "metadata": {},
   "source": [
    "Consider the following environment:\n",
    "\n",
    "<img src=\"images/road_env.jpg\" style=\"zoom: 40%;\"/>\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and must reach the goal in cell $(8,6)$. The agent can move in the four directions (except when a wall is present), and for each step taken the agent receives a negative reward (-0.05).\n",
    "In cells representing roads with intersections, the agent must wait for the traffic light to turn green before proceeding. At busy intersections (indicated by two traffic lights in the same cell), the agent will have to wait a long time to cross the intersection. This implies that if the agent tries to move to another cell, the action has a 20% chance of success, while in the remaining 80% of cases the agent will remain in the cell representing the intersection and will have to try to move again. On the other hand, intersections with only one traffic light are less busy and in this case actions will only have a 20% chance of failing.\n",
    "\n",
    "Consider the problem of computing the optimal policy for the environment reported above and use the provided code to print it. How does the policy change with respect to the discount factor? Analyse and motivate such behaviour.\n",
    "\n",
    "<span style=\"color:green\">La funzione di check implementata non controlla le celle corrispondenti ai muri</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be1e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'R' 'W' 'W' 'W' 'W' 'R' 'W' 'W']\n",
      " ['W' 'Ts' 'R' 'R' 'R' 'R' 'Tl' 'R' 'R']\n",
      " ['W' 'R' 'W' 'W' 'W' 'W' 'R' 'W' 'W']\n",
      " ['R' 'Ts' 'R' 'Ts' 'R' 'R' 'Ts' 'W' 'W']\n",
      " ['W' 'W' 'W' 'R' 'W' 'W' 'R' 'Ts' 'R']\n",
      " ['W' 'R' 'R' 'Tl' 'W' 'W' 'W' 'R' 'W']\n",
      " ['W' 'R' 'W' 'R' 'Ts' 'R' 'R' 'Tl' 'R']\n",
      " ['W' 'R' 'W' 'W' 'R' 'W' 'W' 'R' 'W']\n",
      " ['R' 'Ts' 'R' 'R' 'Tl' 'R' 'G' 'Ts' 'R']]\n",
      "\n",
      "Actions encoding:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Cell type of start state:  S\n",
      "Cell type of goal state:  G\n",
      "Cell type of cell (1, 6):  Tl\n",
      "Cell type of cell (1, 1):  Ts\n",
      "Probability of effectivelty performing action 'R' from cell (1,1) to cell (1,2): 0.8\n"
     ]
    }
   ],
   "source": [
    "import os, sys \n",
    "\n",
    "module_path = os.path.abspath(os.path.join('tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "env_name = 'RoadEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(\"\\nActions encoding: \", env.actions)\n",
    "\n",
    "# Remember that you can know the type of a cell whenever you need by accessing the grid element of the environment:\n",
    "print(\"Cell type of start state: \",env.grid[env.startstate])\n",
    "print(\"Cell type of goal state: \",env.grid[env.goalstate])\n",
    "state = 15 # a very busy intersection\n",
    "print(f\"Cell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "state = 10 # a less busy intersection\n",
    "print(f\"Cell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "print(f\"Probability of effectivelty performing action 'R' from cell (1,1) to cell (1,2): {env.T[state, 1, state+1]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39d99a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, maxiters=500, discount=0.9, max_error=1e-3): \n",
    "\n",
    "    U_1 = [0 for _ in range(environment.observation_space.n)] \n",
    "    delta = 0 \n",
    "    \n",
    "    while True:\n",
    "        maxiters -= 1\n",
    "        U = U_1.copy()\n",
    "        delta = 0\n",
    "        for state in range(environment.observation_space.n):\n",
    "            \n",
    "            max_array = [0 for _ in range(environment.action_space.n)] \n",
    "            for action in range(environment.action_space.n):\n",
    "                for next_state in range(environment.observation_space.n):\n",
    "                    max_array[action] += env.T[state, action, next_state] * U[next_state]\n",
    "                    \n",
    "            if env.grid[state] == \"G\":\n",
    "                U_1[state] = environment.RS[state]\n",
    "            else:           \n",
    "                U_1[state] = environment.RS[state] + discount * max(max_array)\n",
    "                \n",
    "            if abs(U_1[state] - U[state]) > delta: \n",
    "                delta = abs(U_1[state] - U[state])         \n",
    "                \n",
    "        if maxiters <= 0 or delta < (max_error*(1-discount)/discount):\n",
    "            break  \n",
    "    return values_to_policy(np.asarray(U), environment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa046cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXECUTION TIME: \n",
      "0.2866\n",
      "\n",
      "Solution: \n",
      "[['R' 'D' 'L' 'L' 'L' 'L' 'D' 'L' 'L']\n",
      " ['L' 'D' 'L' 'R' 'R' 'R' 'D' 'L' 'L']\n",
      " ['L' 'D' 'L' 'L' 'L' 'L' 'D' 'L' 'L']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'D' 'L' 'L']\n",
      " ['L' 'L' 'L' 'D' 'L' 'L' 'R' 'D' 'L']\n",
      " ['L' 'D' 'L' 'D' 'L' 'L' 'L' 'D' 'L']\n",
      " ['L' 'D' 'L' 'R' 'D' 'L' 'R' 'D' 'L']\n",
      " ['L' 'D' 'L' 'L' 'D' 'L' 'L' 'D' 'L']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'L']]\n",
      "\n",
      "\u001b[1m\u001b[92mYour solution is correct!\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "t = timer()\n",
    "\n",
    "solution = value_iteration(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\\n\".format(round(timer() - t, 4)))\n",
    "\n",
    "solution_render = np.vectorize(env.actions.get)(solution.reshape(env.rows, env.cols))\n",
    "\n",
    "print(\"Solution: \\n{}\\n\".format(solution_render))\n",
    "\n",
    "check_sol(solution_render)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "703075dc036e8ebc3a027aec30cd295176a007527fa40434b7705e84e779ac0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
