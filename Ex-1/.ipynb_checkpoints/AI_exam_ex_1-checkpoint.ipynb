{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c707f35e",
   "metadata": {},
   "source": [
    "# AI Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233976dd",
   "metadata": {},
   "source": [
    "Consider the following environment:\n",
    "\n",
    "<img src=\"images/road_env.jpg\" style=\"zoom: 40%;\"/>\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and must reach the cell $(8,6)$. The agent can move in the four directions (except when a wall is present), and for each step taken the agent receives a negative reward (-0.05).\n",
    "\n",
    "The cell $(8,6)$ provide a positive reward: +5.0. \n",
    "\n",
    "In cells representing roads with intersections, the agent must wait for the traffic light to turn green before proceeding. At busy intersections (indicated by two traffic lights in the same cell), the agent will have to wait a long time to cross the intersection. This implies that if the agent tries to move to another cell, the action has a 20% chance of success, while in the remaining 80% of cases the agent will remain in the cell representing the intersection and will have to try to move again. On the other hand, intersections with only one traffic light are less busy and in this case actions will only have a 20% chance of failing.\n",
    "\n",
    "Consider the problem of computing the optimal sollution for the environment reported above and use the provided code to print it. How does the policy change with respect to the discount factor? Analyse and motivate such behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be1e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'R' 'W' 'W' 'W' 'W' 'R' 'W' 'W']\n",
      " ['W' 'Ts' 'R' 'R' 'R' 'R' 'Tl' 'R' 'R']\n",
      " ['W' 'R' 'W' 'W' 'W' 'W' 'R' 'W' 'W']\n",
      " ['R' 'Ts' 'R' 'Ts' 'R' 'R' 'Ts' 'W' 'W']\n",
      " ['W' 'W' 'W' 'R' 'W' 'W' 'R' 'Ts' 'R']\n",
      " ['W' 'R' 'R' 'Tl' 'W' 'W' 'W' 'R' 'W']\n",
      " ['W' 'R' 'W' 'R' 'Ts' 'R' 'R' 'Tl' 'R']\n",
      " ['W' 'R' 'W' 'W' 'R' 'W' 'W' 'R' 'W']\n",
      " ['R' 'Ts' 'R' 'R' 'Tl' 'R' 'G' 'Ts' 'R']]\n",
      "\n",
      "Actions encoding:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Cell type of start state:  S\n",
      "Cell type of goal state: G Reward: 5.0\n",
      "Cell type of cell (1, 6):  Tl\n",
      "Probability of effectivelty performing action 'R' from cell (1,6) to cell (1,7): 0.2\n",
      "Cell type of cell (1, 1):  Ts\n",
      "Probability of effectivelty performing action 'R' from cell (1,1) to cell (1,2): 0.8\n"
     ]
    }
   ],
   "source": [
    "import os, sys \n",
    "\n",
    "module_path = os.path.abspath(os.path.join('tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "env_name = 'RoadEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(\"\\nActions encoding: \", env.actions)\n",
    "\n",
    "# Remember that you can know the type of a cell whenever you need by accessing the grid element of the environment:\n",
    "print(\"Cell type of start state: \",env.grid[env.startstate])\n",
    "print(\"Cell type of goal state: {} Reward: {}\".format(env.grid[env.goalstate],env.RS[env.goalstate]))\n",
    "state = 15 # a very busy intersection\n",
    "print(f\"Cell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "print(f\"Probability of effectivelty performing action 'R' from cell (1,6) to cell (1,7): {env.T[state, 1, state+1]}\") \n",
    "state = 10 # a less busy intersection\n",
    "print(f\"Cell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "print(f\"Probability of effectivelty performing action 'R' from cell (1,1) to cell (1,2): {env.T[state, 1, state+1]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39d99a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPLEMENT THIS FUNCTION, YOU CAN CHANGE THE PARAMETERS FOR THE FUNCTION IF THIS IS USEFUL\n",
    "def my_solution(environment): \n",
    "    return np.random.choice(environment.action_space.n,environment.observation_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aa046cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXECUTION TIME: \n",
      "0.0003\n",
      "\n",
      "Solution: \n",
      "[['R' 'U' 'U' 'R' 'L' 'D' 'R' 'U' 'D']\n",
      " ['R' 'R' 'D' 'U' 'U' 'U' 'U' 'D' 'D']\n",
      " ['U' 'L' 'D' 'D' 'D' 'L' 'L' 'R' 'D']\n",
      " ['D' 'U' 'L' 'U' 'D' 'D' 'L' 'R' 'D']\n",
      " ['D' 'D' 'D' 'U' 'L' 'D' 'R' 'U' 'U']\n",
      " ['R' 'D' 'R' 'U' 'L' 'L' 'U' 'U' 'D']\n",
      " ['D' 'D' 'D' 'U' 'D' 'D' 'R' 'R' 'R']\n",
      " ['U' 'U' 'D' 'L' 'L' 'D' 'L' 'D' 'U']\n",
      " ['D' 'L' 'D' 'D' 'R' 'D' 'U' 'U' 'R']]\n",
      "\n",
      "\u001b[91mYour solution is not correct\n"
     ]
    }
   ],
   "source": [
    "t = timer()\n",
    "\n",
    "solution = my_solution(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\\n\".format(round(timer() - t, 4)))\n",
    "\n",
    "solution_render = np.vectorize(env.actions.get)(solution.reshape(env.rows, env.cols))\n",
    "\n",
    "print(\"Solution: \\n{}\\n\".format(solution_render))\n",
    "\n",
    "check_sol(solution_render)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "703075dc036e8ebc3a027aec30cd295176a007527fa40434b7705e84e779ac0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
